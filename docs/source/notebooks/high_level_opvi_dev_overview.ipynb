{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operator Variational Inference (OPVI)\n",
    "\n",
    "Inspired by\n",
    "\n",
    "    Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei\n",
    "    \"Operator Variational Inference\", https://arxiv.org/abs/1610.09033\n",
    "\n",
    "\n",
    "To so long time had passed since paper above was published when OPVI bacame a new paradigm for all VI stuff in PyMC3. Why is it so expressive and covers all methods exist in literature? The idea behind OPVI is modularity.\n",
    "\n",
    "It generalizes variational inverence so that the problem is build with blocks. The first and essential block is **Model** itself. Second is **Approximation**, in some cases $log Q(D)$ is not really needed. Necessity depends on the third and forth part of that black box, **Operator** and **Test Function** respectively.\n",
    "\n",
    "Operator is like an approach we use, it constructs loss from given Model, Approximation and Test Function. The last one is not needed if we minimize KL Divergence from Q to posterior. As a drawback we need to compute $loq Q(D)$. Sometimes approximation family is intractable and $loq Q(D)$ is not available, here comes *LS(Langevin Stein) Operator* with a set of test functions or *KSD* for taking gradients needed for inference.\n",
    "\n",
    "Test Function has more unintuitive meaning. It is usually used with LS operator and represents all we want from our approximate distribution. For any given vector based function of $z$ LS operator yields zero mean function under posterior. $loq Q(D)$ is no more needed. That opens a door to rich approximation families as neural networks.\n",
    "\n",
    "Not only [ADVI](https://arxiv.org/abs/1506.03431) and [Langevin Stein Operator](https://arxiv.org/abs/1610.09033) VI are applicable with OPVI framework. [Normalizing Flows](https://arxiv.org/abs/1505.05770) / [SVGD](https://arxiv.org/abs/1608.04471) / [ASVGD](http://bayesiandeeplearning.org/papers/BDL_21.pdf) fit well for it.\n",
    "\n",
    "There are a lot of new papers like the following list that are very interesting to read and inspire for more contributions.\n",
    "\n",
    "* [Gradient Estimators for Implicit Models](https://arxiv.org/abs/1705.07107)\n",
    "* [Improving Variational Auto-Encoders using Householder Flow](https://arxiv.org/abs/1611.09630)\n",
    "* [Black-box Importance Sampling](https://arxiv.org/abs/1610.05247)\n",
    "* [Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning](https://arxiv.org/abs/1611.01722)\n",
    "\n",
    "Moreover there is an approach for [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) (AEVB) that allows to do great things.\n",
    "\n",
    "So taking it all there are not so many methods exist\n",
    "\n",
    "| Method                     | pymc3       |\n",
    "|----------------------------|-------------|\n",
    "| ADVI                       | +           |\n",
    "| FullRankADVI               | +           |\n",
    "| Normalizing Flows          | coming soon |\n",
    "| Langevin Stein Operator VI | -           |\n",
    "| SVGD                       | +           |\n",
    "| ASVGD                      | +           |\n",
    "\n",
    "As you see it PyMC3 has a great support for variational inference with both scalable (ADVI) and accurate methods (SVGD). Internals are tricky to make it all work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "As mentioned before OPVI has Model, TestFuntions, Operator and Approximation. The essential part is ofc `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pymc3.model.Model"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is it\n",
    "pm.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is nothing tricky here. We all use this class to define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<property at 0x1146c1ef8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.Model.logpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This property returns a tensor that we can later use for constructing loss functions in VI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operators\n",
    "The essential code for OPVI is in variatioanal module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pymc3.variational.opvi' from '/Users/ferres/dev/pymc3/pymc3/variational/opvi.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.variational.opvi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find base classes for Approximations, Operators, Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base class for Operator\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    approx : :class:`Approximation`\n",
      "        an approximation instance\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    For implementing Custom operator it is needed to define :func:`Operator.apply` method\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(pm.variational.opvi.Operator.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator itself\n",
      "\n",
      "        .. math::\n",
      "\n",
      "            (O^{p,q}f_{\\theta})(z)\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        f : :class:`TestFunction` or None\n",
      "            function that takes `z = self.input` and returns\n",
      "            same dimensional output\n",
      "\n",
      "        nmc : n\n",
      "            monte carlo samples to use\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        `TensorVariable`\n",
      "            symbolically applied operator\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(pm.variational.opvi.Operator.apply.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pymc3 we have three of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Operator based on Kullback Leibler Divergence\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        KL[q(v)||p(v)] = \\int q(v)\\log\\frac{q(v)}{p(v)}dv\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(pm.variational.operators.KL.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Operator based on Kernelized Stein Discrepancy\n",
      "\n",
      "    *Input:* A target distribution with density function :math:`p(x)`\n",
      "        and a set of initial particles :math:`\\{x^0_i\\}^n_{i=1}`\n",
      "\n",
      "    *Output:* A set of particles :math:`\\{x_i\\}^n_{i=1}` that approximates the target distribution.\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        x_i^{l+1} \\leftarrow \\epsilon_l \\hat{\\phi}^{*}(x_i^l) \\\\\n",
      "        \\hat{\\phi}^{*}(x) = \\frac{1}{n}\\sum^{n}_{j=1}[k(x^l_j,x) \\nabla_{x^l_j} logp(x^l_j)+ \\nabla_{x^l_j} k(x^l_j,x)]\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    approx : :class:`Empirical`\n",
      "        Empirical Approximation used for inference\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    -   Qiang Liu, Dilin Wang (2016)\n",
      "        Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm\n",
      "        arXiv:1608.04471\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(pm.variational.operators.KSD.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Amortized Stein Variational Gradient Descent\n",
      "\n",
      "    This inference is based on Kernelized Stein Discrepancy\n",
      "    it's main idea is to move initial noisy particles so that\n",
      "    they fit target distribution best.\n",
      "\n",
      "    Algorithm is outlined below\n",
      "\n",
      "    *Input:* Parametrized random generator :math:`R_{\\theta}`\n",
      "\n",
      "    *Output:* :math:`R_{\\theta^{*}}` that approximates the target distribution.\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        \\Delta x_i &= \\hat{\\phi}^{*}(x_i) \\\\\n",
      "        \\hat{\\phi}^{*}(x) &= \\frac{1}{n}\\sum^{n}_{j=1}[k(x_j,x) \\nabla_{x_j} logp(x_j)+ \\nabla_{x_j} k(x_j,x)] \\\\\n",
      "        \\Delta_{\\theta} &= \\frac{1}{n}\\sum^{n}_{i=1}\\Delta x_i\\frac{\\partial x_i}{\\partial \\theta}\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    -   Dilin Wang, Yihao Feng, Qiang Liu (2016)\n",
      "        Learning to Sample Using Stein Discrepancy\n",
      "        http://bayesiandeeplearning.org/papers/BDL_21.pdf\n",
      "\n",
      "    -   Dilin Wang, Qiang Liu (2016)\n",
      "        Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning\n",
      "        arXiv:1611.01722\n",
      "\n",
      "    -   Yang Liu, Prajit Ramachandran, Qiang Liu, Jian Peng (2017)\n",
      "        Stein Variational Policy Gradient\n",
      "        arXiv:1704.02399\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(pm.variational.operators.AKSD.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximations\n",
    "\n",
    "Approximations is another core part for OPVI. To make them flexible and scalable we use modular structure there too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base class for approximations.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    local_rv : dict[var->tuple]\n",
      "        mapping {model_variable -> local_variable (:math:`\\mu`, :math:`\\rho`)}\n",
      "        Local Vars are used for Autoencoding Variational Bayes\n",
      "        See (AEVB; Kingma and Welling, 2014) for details\n",
      "    model : :class:`Model`\n",
      "        PyMC3 model for inference\n",
      "    cost_part_grad_scale : float or scalar tensor\n",
      "        Scaling score part of gradient can be useful near optimum for\n",
      "        archiving better convergence properties. Common schedule is\n",
      "        1 at the start and 0 in the end. So slow decay will be ok.\n",
      "        See (Sticking the Landing; Geoffrey Roeder,\n",
      "        Yuhuai Wu, David Duvenaud, 2016) for details\n",
      "    scale_cost_to_minibatch : bool, default False\n",
      "        Scale cost to minibatch instead of full dataset\n",
      "    random_seed : None or int\n",
      "        leave None to use package global RandomStream or other\n",
      "        valid value to create instance specific one\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    Defining an approximation needs\n",
      "    custom implementation of the following methods:\n",
      "\n",
      "        - :code:`.create_shared_params(**kwargs)`\n",
      "            Returns dict\n",
      "\n",
      "        - :code:`symbolic_random_global_matrix` node property\n",
      "            It takes internally `symbolic_initial_global_matrix`\n",
      "            and performs appropriate transforms. To memoize result\n",
      "            one should use :code:`@node_property` wrapper instead :code:`@property`.\n",
      "            Returns TensorVariable\n",
      "\n",
      "        - :code:`.symbolic_log_q_W_global` node property\n",
      "            Should use vectorized form if possible and return vector of size `(s,)`\n",
      "            It is needed only if used with operator that requires :math:`logq`\n",
      "            of an approximation. Returns vector\n",
      "\n",
      "    You can also override the following methods:\n",
      "\n",
      "        -   :code:`.check_model(model, **kwargs)`\n",
      "            Do some specific check for model having `kwargs`\n",
      "\n",
      "    `kwargs` mentioned above are supplied as additional arguments\n",
      "    for :class:`Approximation`\n",
      "\n",
      "    There are some defaults class attributes for approximation classes that can be\n",
      "    optionally overridden.\n",
      "\n",
      "        -   :code:`initial_dist_local_name = 'normal'`\n",
      "            :code:`initial_dist_global_name = 'normal'`\n",
      "            string that represents name of the initial distribution.\n",
      "            In most cases if will be `uniform` or `normal`\n",
      "\n",
      "        -   :code:`initial_dist_local_map = 0.\n",
      "            :code:`initial_dist_global_map = 0.`\n",
      "            point where initial distribution has maximum density\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    -   Geoffrey Roeder, Yuhuai Wu, David Duvenaud, 2016\n",
      "        Sticking the Landing: A Simple Reduced-Variance Gradient for ADVI\n",
      "        approximateinference.org/accepted/RoederEtAl2016.pdf\n",
      "\n",
      "    -   Kingma, D. P., & Welling, M. (2014).\n",
      "        Auto-Encoding Variational Bayes. stat, 1050, 1.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(pm.variational.opvi.Approximation.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we subclass and define more approximations in a separate module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MeanField', 'FullRank', 'Empirical', 'sample_approx']\n"
     ]
    }
   ],
   "source": [
    "print(pm.variational.approximations.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Functions\n",
    "\n",
    "Not all our operators use test functions. So we yet have the only one kernel for SVGD/ASVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rbf']\n"
     ]
    }
   ],
   "source": [
    "print(pm.variational.test_functions.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Combining it all we can construct an inference method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3.variational.inference import Inference\n",
    "from pymc3.variational.operators import KL, KSD, AKSD\n",
    "from pymc3.variational.approximations import MeanField, FullRank, Empirical\n",
    "\n",
    "with pm.Model() as model:\n",
    "    norm = pm.Normal('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Inference in module pymc3.variational.inference:\n",
      "\n",
      "class Inference(builtins.object)\n",
      " |  Base class for Variational Inference\n",
      " |  \n",
      " |  Communicates Operator, Approximation and Test Function to build Objective Function\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  op : Operator class\n",
      " |  approx : Approximation class or instance\n",
      " |  tf : TestFunction instance\n",
      " |  local_rv : dict\n",
      " |      mapping {model_variable -> local_variable}\n",
      " |      Local Vars are used for Autoencoding Variational Bayes\n",
      " |      See (AEVB; Kingma and Welling, 2014) for details\n",
      " |  model : Model\n",
      " |      PyMC3 Model\n",
      " |  op_kwargs : dict\n",
      " |      kwargs passed to :class:`Operator`\n",
      " |  kwargs : kwargs\n",
      " |      additional kwargs for :class:`Approximation`\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, op, approx, tf, local_rv=None, model=None, op_kwargs=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, n=10000, score=None, callbacks=None, progressbar=True, **kwargs)\n",
      " |      Performs Operator Variational Inference\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int\n",
      " |          number of iterations\n",
      " |      score : bool\n",
      " |          evaluate loss on each iteration or not\n",
      " |      callbacks : list[function : (Approximation, losses, i) -> None]\n",
      " |          calls provided functions after each iteration step\n",
      " |      progressbar : bool\n",
      " |          whether to show progressbar or not\n",
      " |      kwargs : kwargs\n",
      " |          additional kwargs for :func:`ObjectiveFunction.step_function`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Approximation\n",
      " |  \n",
      " |  run_profiling(self, n=1000, score=None, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  approx\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  APPROX = None\n",
      " |  \n",
      " |  OP = None\n",
      " |  \n",
      " |  TF = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can construct inference by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymc3.variational.opvi.ObjectiveFunction at 0x11cf1e6d8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference = Inference(\n",
    "    model=model,\n",
    "    op=KL,\n",
    "    approx=MeanField,\n",
    "    tf=None\n",
    ")\n",
    "inference.objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 0.0024892: 100%|██████████| 10000/10000 [00:00<00:00, 11669.79it/s] \n",
      "Finished [100%]: Average Loss = 0.0025053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymc3.variational.approximations.MeanField at 0x117708198>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or get output for objective function in the same way. Recall the formula from `Operator.apply` docstring\n",
    " $$objective = (O^{p,q}f_{\\theta})(z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    objective = KL(MeanField())(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymc3.variational.opvi.ObjectiveFunction at 0x11ddf6940>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.09470486640930176, dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective(nmc=10).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see this modularity allows to create separable code for OPVI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
