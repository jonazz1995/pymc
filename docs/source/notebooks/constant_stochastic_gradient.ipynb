{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Can we approximately sample from a Bayesian posterior distribution if we are only allowed to touch a small mini-batch of data-items for every sample we generate ?`\n",
    "\n",
    "Based on results from a recent paper, a simple implementation of constant stochastic gradient is shown to have approximately sampling from the posterior. The goal of tand compare the approximate distribution to iterates from NUTS and SGFS.\n",
    "\n",
    "Ref: https://arxiv.org/abs/1704.04289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import warnings\n",
    "import theano\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import theano.tensor as tt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling: Constant Stochastic Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: A multivariate regression problem on the Protein Structure Properties dataset available at the [uci repo](https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-11-26 17:48:19--  https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv\n",
      "Resolving archive.ics.uci.edu... 128.195.10.249\n",
      "Connecting to archive.ics.uci.edu|128.195.10.249|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3528710 (3.4M) [text/csv]\n",
      "Saving to: ‘/tmp/CASP.csv.4’\n",
      "\n",
      "CASP.csv.4          100%[===================>]   3.37M  1.41MB/s    in 2.4s    \n",
      "\n",
      "2017-11-26 17:48:22 (1.41 MB/s) - ‘/tmp/CASP.csv.4’ saved [3528710/3528710]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv --directory-prefix=/tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('/tmp/CASP.csv', delimiter=',')\n",
    "data = (raw_data - raw_data.mean())/raw_data.std()\n",
    "q_size = data.shape[1]-1\n",
    "q_name = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_N = data.size/len(data.columns)\n",
    "train_test_split = 0.95\n",
    "ixs = rng.randint(data_N, size=int(data_N*train_test_split))\n",
    "neg_ixs = list(set(range(data_N)) - set(ixs))\n",
    "train_df = data.iloc[ixs]\n",
    "test_df = data.iloc[neg_ixs]\n",
    "\n",
    "N = train_df.size / len(train_df.columns)\n",
    "n_test = test_df.size / len(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_df[q_name].as_matrix()\n",
    "train_Y = train_df['RMSD'].as_matrix()\n",
    "\n",
    "test_X = test_df[q_name].as_matrix()\n",
    "test_Y = test_df['RMSD'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try regression models from sklearn to construct the best pymc3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BayesianRidge Mean Absolute Error is 0.705775225403\n",
      "The OLS Mean Absolute Error is 0.705739946132\n",
      "The Ridge Mean Absolute Error is 0.705748180444\n",
      "The Lasso Mean Absolute Error is 0.798798476908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashank/.virtualenvs/pymc3-dev/lib/python2.7/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "sklearn_regression_model = {\n",
    "    'Ridge': linear_model.Ridge (alpha = .5),\n",
    "    'Lasso': linear_model.Lasso(alpha = 0.1),\n",
    "    'BayesianRidge': linear_model.BayesianRidge(),\n",
    "    'OLS': linear_model.LinearRegression(),\n",
    "}\n",
    "for name, reg in sklearn_regression_model.items():\n",
    "    reg.fit(train_X, train_Y) \n",
    "    pred = reg.predict(test_X)\n",
    "    diff = pred-test_Y\n",
    "    print('The {} Mean Absolute Error is {}'.format(name, np.sum(np.abs(diff))/test_Y.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS fit has the minimum mean absolute error so we will select normal priors on the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = theano.shared(train_X, name='X')\n",
    "model_output = theano.shared(train_Y, name='Y')\n",
    "\n",
    "with pm.Model() as model:\n",
    "    b0 = pm.Normal(\"Intercept\", mu=0.0, sd=1.0)\n",
    "    b1 = pm.Normal(\"Slope\", mu=0.0, shape=(q_size,))\n",
    "    std = pm.HalfNormal(\"std\", sd=1.0)\n",
    "\n",
    "    mu = b0 + theano.dot(model_input, b1)      \n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sd=std, observed=model_output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 54415.487519\n",
      "         Iterations: 20\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 61\n",
      "The OLS MAP Estimate Mean Absolute Error is 0.70575174425\n"
     ]
    }
   ],
   "source": [
    "with model:\n",
    "    start = pm.find_MAP()\n",
    "    \n",
    "map_pred = np.matmul(test_X, start['Slope'] ) + start['Intercept']\n",
    "map_diff = map_pred-test_Y\n",
    "print('The {} Mean Absolute Error is {}'.format(\"OLS MAP Estimate\", np.sum(np.abs(map_diff))/test_Y.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will to proceed perform bayesian sampling to calculate the posterior on the OLS pymc3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make model and minibatches input for the stochastic sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "100%|██████████| 10500/10500 [11:20<00:00, 15.44it/s]\n"
     ]
    }
   ],
   "source": [
    "draws = 10000\n",
    "with model:\n",
    "    nuts_trace = pm.sample(draws=draws, tune=500)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator that returns mini-batches in each iteration\n",
    "def create_minibatches(batch_size):\n",
    "    while True:\n",
    "        # Return random data samples of set size 100 each iteration\n",
    "        ixs = rng.randint(N, size=batch_size)\n",
    "        yield (train_X[ixs], train_Y[ixs])\n",
    "\n",
    "# Tensors and RV that wil l be using mini-batches\n",
    "batch_size = 50\n",
    "minibatches = create_minibatches(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashank/.virtualenvs/pymc3-dev/lib/python2.7/site-packages/pymc3-3.1-py2.7.egg/pymc3/step_methods/sgmcmc.py:112: UserWarning: Warning: Stochastic Gradient based sampling methods are experimental step methods and not yet recommended for use in PyMC3!\n",
      "100%|██████████| 50500/50500 [01:17<00:00, 651.88it/s]\n"
     ]
    }
   ],
   "source": [
    "model_input = theano.shared(train_X, name='X')\n",
    "model_output = theano.shared(train_Y, name='Y')\n",
    "\n",
    "with pm.Model() as model:\n",
    "    b0 = pm.Normal(\"Intercept\", mu=0.0, sd=1.0)\n",
    "    b1 = pm.Normal(\"Slope\", mu=0.0, shape=(q_size,))\n",
    "    std = pm.HalfNormal(\"std\", sd=1.0)\n",
    "\n",
    "    mu = b0 + theano.dot(model_input, b1)      \n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sd=std, observed=model_output) \n",
    "    \n",
    "minibatch_tensors = [model_input, model_output]\n",
    "\n",
    "draws = 10000*5\n",
    "with model:\n",
    "    csg_step_method = pm.step_methods.CSG(vars=model.vars,\n",
    "                                          model=model,\n",
    "                                          total_size=N, \n",
    "                                          batch_size=batch_size,\n",
    "                                          minibatches=minibatches, \n",
    "                                          minibatch_tensors=minibatch_tensors) \n",
    "    csg_trace = pm.sample(draws=draws, step=csg_step_method, tune=500, init='map')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 17935/50500 [00:40<01:13, 441.40it/s]"
     ]
    }
   ],
   "source": [
    "model_input = theano.shared(train_X, name='X')\n",
    "model_output = theano.shared(train_Y, name='Y')\n",
    "\n",
    "with pm.Model() as model:\n",
    "    b0 = pm.Normal(\"Intercept\", mu=0.0, sd=1.0)\n",
    "    b1 = pm.Normal(\"Slope\", mu=0.0, shape=(q_size,))\n",
    "    std = pm.HalfNormal(\"std\", sd=1.0)\n",
    "\n",
    "    mu = b0 + theano.dot(model_input, b1)      \n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sd=std, observed=model_output) \n",
    "    \n",
    "minibatch_tensors = [model_input, model_output]\n",
    "\n",
    "draws = 10000*5\n",
    "with model:\n",
    "    sgfs_step_method = pm.step_methods.SGFS(vars=model.vars,\n",
    "                                            step_size=0.1,\n",
    "                                            step_size_decay=1000,\n",
    "                                            total_size=N,\n",
    "                                            batch_size=batch_size,\n",
    "                                            minibatches=minibatches, \n",
    "                                            minibatch_tensors=minibatch_tensors)  \n",
    "    sgfs_trace = pm.sample(draws=draws, step=sgfs_step_method, tune=500, init='map')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUTS Trace Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(nuts_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preconditioned CSG Trace Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(csg_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGFS Trace Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(sgfs_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace shared variables with testing set\n",
    "model_input.set_value(test_X)\n",
    "model_output.set_value(test_Y)\n",
    "\n",
    "samples = 1000\n",
    "\n",
    "# Creater posterior predictive samples\n",
    "sgfs_ppc = pm.sample_ppc(sgfs_trace, model=model, samples=samples, random_seed=0)\n",
    "sgfs_pred = sgfs_ppc['y_obs'].mean(axis=0)\n",
    "\n",
    "# Creater posterior predictive samples\n",
    "csg_ppc = pm.sample_ppc(csg_trace, model=model, samples=samples, random_seed=0)\n",
    "csg_pred = csg_ppc['y_obs'].mean(axis=0)\n",
    "\n",
    "# Nuts predictive samples\n",
    "nuts_ppc = pm.sample_ppc(nuts_trace, model=model, samples=samples, random_seed=0)\n",
    "nuts_pred = nuts_ppc['y_obs'].mean(axis=0)\n",
    "\n",
    "sgfs_diff = sgfs_pred-test_Y\n",
    "csg_diff = csg_pred-test_Y\n",
    "nuts_diff = nuts_pred-test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The NUTS Mean Absolute Error is {}'.format(np.sum(np.abs(nuts_diff))/test_Y.size))\n",
    "print('The CSG Mean Absolute Error is {}'.format(np.sum(np.abs(csg_diff))/test_Y.size))\n",
    "print('The SGFS Mean Absolute Error is {}'.format(np.sum(np.abs(sgfs_diff))/test_Y.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error for all the sampling algorithms is ~ 0.706. which is very close to the ols map fit\n",
    "0.7057. The error is slightly better using SGFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.tsplot(data=nuts_diff, ax=ax, color=\"r\", interpolate=False, alpha=0.3)\n",
    "sns.tsplot(data=csg_diff, ax=ax, color=\"g\", interpolate=False, alpha=0.3)\n",
    "sns.tsplot(data=sgfs_diff, ax=ax, color=\"b\", interpolate=False, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample covariance projections on the smallest and largest components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_covariance(step_method, trace):\n",
    "    bij = pm.DictToArrayBijection(step_method.ordering, step_method.model.test_point)\n",
    "    q_size = bij.map(step_method.model.model.test_point).size\n",
    "    sample_size = len(trace)\n",
    "    posterior = np.empty((q_size, sample_size))\n",
    "    for index, point in enumerate(trace):\n",
    "        posterior[:, index] = bij.map(point)\n",
    "    posterior_minus_mean = posterior - np.asmatrix(posterior.mean(axis=1)).T\n",
    "    normalized_posterior = posterior_minus_mean / np.asmatrix(posterior.std(axis=1)).T\n",
    "    cov = np.matmul(normalized_posterior, normalized_posterior.T)\n",
    "    return posterior_minus_mean, cov\n",
    "\n",
    "def projection(posterior, cov):\n",
    "    U, S, V_h = np.linalg.svd(a=cov, compute_uv=True, full_matrices=True)\n",
    "    first_projection = V_h[0, :]\n",
    "    last_projection = V_h[-1, :]\n",
    "    q_size, samples = posterior.shape\n",
    "    projection_matrix = np.empty((samples, 2))\n",
    "    for i in range(samples):\n",
    "        projection_matrix[i, 0] = np.matmul(first_projection, posterior_minus_mean[:, i])\n",
    "        projection_matrix[i, 1] = np.matmul(last_projection, posterior_minus_mean[:, i])\n",
    "    return projection_matrix\n",
    "\n",
    "burn_in = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between CSG and NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "posterior_minus_mean, cov = posterior_covariance(csg_step_method, nuts_trace[burn_in:])\n",
    "projection_matrix = projection(posterior_minus_mean, cov)\n",
    "df_nuts = pd.DataFrame(projection_matrix, columns=['X', 'Y'])\n",
    "ax.scatter(x=df_nuts['X'], y=df_nuts['Y'], s=0.5, alpha=0.1, c='r')\n",
    "\n",
    "posterior_minus_mean, cov = posterior_covariance(csg_step_method, csg_trace[burn_in:])\n",
    "projection_matrix = projection(posterior_minus_mean, cov)\n",
    "df_csg = pd.DataFrame(projection_matrix, columns=['X', 'Y'])\n",
    "ax.scatter(x=df_csg['X'], y=df_csg['Y'], s=0.5, alpha=0.1, c='g')\n",
    "\n",
    "\n",
    "ax.set_title(\"Stationary sampling distributions of the iterates of CSG and NUTS\", fontsize=20, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between SGFS and NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "posterior_minus_mean, cov = posterior_covariance(csg_step_method, nuts_trace[burn_in:])\n",
    "projection_matrix = projection(posterior_minus_mean, cov)\n",
    "df_nuts = pd.DataFrame(projection_matrix, columns=['X', 'Y'])\n",
    "ax.scatter(x=df_nuts['X'], y=df_nuts['Y'], s=0.5, alpha=0.2, c='r')\n",
    "\n",
    "posterior_minus_mean, cov = posterior_covariance(sgfs_step_method, sgfs_trace[burn_in:])\n",
    "projection_matrix = projection(posterior_minus_mean, cov)\n",
    "df_sgfs = pd.DataFrame(projection_matrix, columns=['X', 'Y'])\n",
    "ax.scatter(x=df_sgfs['X'], y=df_sgfs['Y'], s=0.5, alpha=0.2, c='b')\n",
    "\n",
    "ax.set_title(\"stationary sampling distributions of the iterates of SGFS and NUTS\", fontsize=20, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant Stochastic Gradient is a good approximator of the posterior, as can be seen from the trace plots and the projections of its sample covariance matrix which \n",
    "largely overlap with NUTS. The paper also presents a figure which shows large overlap between the true posterior and the constant stochastic gradient iteratre distribution. In comparison SGFS has a unique distribution, which shows a different relationship between the two components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
